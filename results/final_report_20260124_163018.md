# Multi-Model Entity Extraction Evaluation Report

**Generated:** 2026-01-24 16:30:18

---

## Executive Summary

This report presents a comprehensive evaluation of multiple entity extraction models 
across two legal contract datasets: CUAD (English) and di2win (Portuguese).


## 1. CUAD Dataset Results (English Legal Contracts)

### 1.1 Model Comparison

| Model | Type | Precision | Recall | F1 Score | Avg Latency (s) |
|-------|------|-----------|--------|----------|-----------------|
| GPT-4o | llm | 0.7444 | 0.8513 | **0.7943** | 4.61 |
| RAG-GPT4o | rag | 0.7389 | 0.8308 | **0.7822** | 2.77 |
| GPT-4-Turbo | llm | 0.7113 | 0.8458 | **0.7727** | 12.44 |
| Gemini-2.0-Flash | llm | 0.6926 | 0.8408 | **0.7596** | 1.71 |

**Best Model:** GPT-4o with F1=0.7943


### 1.2 Per-Entity Performance (GPT-4o)

| Entity Type | Precision | Recall | F1 Score | Support |
|-------------|-----------|--------|----------|---------|
| DOC_NAME | 0.8387 | 0.8966 | 0.8667 | 29 |
| PARTY | 0.7378 | 0.8832 | 0.8040 | 137 |
| AGMT_DATE | 0.6786 | 0.6552 | 0.6667 | 29 |


## 2. di2win Dataset Results (Portuguese Legal Contracts)

### 2.1 Model Comparison

| Model | Type | Precision | Recall | F1 Score | Avg Latency (s) |
|-------|------|-----------|--------|----------|-----------------|
| GPT-4o | llm | 0.3948 | 0.3375 | **0.3639** | 15.97 |
| Gemini-2.0-Flash | llm | 0.3826 | 0.3383 | **0.3591** | 4.62 |
| GPT-4-Turbo | llm | 0.3709 | 0.3027 | **0.3333** | 51.04 |
| RAG-GPT4o | rag | 0.3904 | 0.2908 | **0.3333** | 5.85 |

**Best Model:** GPT-4o with F1=0.3639


### 2.2 Per-Entity Performance (GPT-4o)

| Entity Type | Precision | Recall | F1 Score | Support |
|-------------|-----------|--------|----------|---------|
| data_de_registro_do_contrato | 0.7143 | 0.7692 | 0.7407 | 13 |
| cnpj->sociedade | 0.9286 | 0.5417 | 0.6842 | 24 |
| nire->sociedade | 0.8333 | 0.4762 | 0.6061 | 21 |
| rg->socio | 0.3750 | 0.5143 | 0.4337 | 35 |
| nome->socio | 0.5600 | 0.3415 | 0.4242 | 82 |
| cep->sociedade | 0.2667 | 0.2105 | 0.2353 | 19 |
| cep->socio | 0.2286 | 0.2105 | 0.2192 | 38 |
| cpf->socio | 0.2000 | 0.2174 | 0.2083 | 46 |
| municipio_(ou_cidade)->socio | 0.2222 | 0.1905 | 0.2051 | 21 |
| municipio_(ou_cidade)->sociedade | 0.1333 | 0.1111 | 0.1212 | 18 |


## 3. Cross-Dataset Analysis

### 3.1 Performance Comparison

| Model | CUAD F1 | di2win F1 | Difference |
|-------|---------|-----------|------------|
| GPT-4-Turbo | 0.7727 | 0.3333 | +0.4394 |
| GPT-4o | 0.7943 | 0.3639 | +0.4304 |
| Gemini-2.0-Flash | 0.7596 | 0.3591 | +0.4005 |
| RAG-GPT4o | 0.7822 | 0.3333 | +0.4489 |

### 3.2 Key Findings

1. **Language Impact:** All models show significantly better performance on English (CUAD) 
compared to Portuguese (di2win) contracts, with an average F1 difference of ~0.40.

2. **Model Ranking:** GPT-4o consistently achieves the highest F1 scores on both datasets.

3. **Speed vs Quality Trade-off:** Gemini-2.0-Flash offers the fastest inference 
while maintaining competitive F1 scores.

4. **RAG Performance:** RAG-enhanced extraction shows comparable performance to 
direct LLM extraction on CUAD but slightly lower on di2win.


## 4. Recommendations

1. **For English Contracts (CUAD-like):** Use GPT-4o for best quality, or Gemini-2.0-Flash for speed-sensitive applications.

2. **For Portuguese Contracts (di2win-like):** Consider fine-tuning domain-specific models or using few-shot prompting with examples.

3. **Future Work:** 

   - Fine-tune SLM models (BERT/RoBERTa) on labeled data

   - Implement hybrid approaches combining RAG with LLM extraction

   - Investigate entity-specific prompt optimization


## 5. Methodology

- **Evaluation Metrics:** Precision, Recall, F1 Score (exact match)

- **Models Evaluated:** GPT-4o, GPT-4-Turbo, Gemini-2.0-Flash, RAG-GPT4o

- **Datasets:**

  - CUAD: Commercial contracts in English (entities: PARTY, DOC_NAME, AGMT_DATE)

  - di2win: Social contracts in Portuguese (entities: nome->socio, cpf, cnpj, etc.)
